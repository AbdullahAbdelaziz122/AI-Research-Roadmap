<!DOCTYPE html>
<html lang="en" data-theme="dark">
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<title>AI/ML PhD + Senior Engineer Roadmap</title>
<link rel="preconnect" href="https://fonts.googleapis.com">
<link href="https://fonts.googleapis.com/css2?family=Space+Mono:ital,wght@0,400;0,700;1,400&family=Syne:wght@400;600;700;800&family=DM+Mono:wght@300;400;500&display=swap" rel="stylesheet">
<link rel="stylesheet" href="styles.css">
</head>
<body>

<!-- ‚ïê‚ïê‚ïê‚ïê THEME TOGGLE ‚ïê‚ïê‚ïê‚ïê -->
<button class="theme-toggle" id="themeToggle" aria-label="Toggle theme">
  <span class="toggle-icon" id="toggleIcon">‚òÄÔ∏è</span>
  <span id="toggleLabel">Light</span>
</button>

<div class="container">

  <!-- HEADER -->
  <header>
    <div class="badge">// FAST-TRACK ROADMAP 2025‚Äì2028</div>
    <h1>AI/ML PhD Level<br>+ Senior Engineer</h1>
    <p class="subtitle">From mathematical foundations to the research frontier ‚Äî a complete, phased execution plan for mastering the full stack of modern artificial intelligence.</p>
    <div class="stats-row">
      <div class="stat"><span class="stat-val">5</span><span class="stat-label">Phases</span></div>
      <div class="stat"><span class="stat-val">~3yr</span><span class="stat-label">Fast Track</span></div>
      <div class="stat"><span class="stat-val">100+</span><span class="stat-label">Core Papers</span></div>
      <div class="stat"><span class="stat-val">PhD</span><span class="stat-label">Target Level</span></div>
    </div>
  </header>

  <!-- ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê -->
  <!-- PHASE 0 ‚Äî MATH BEDROCK -->
  <!-- ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê -->
  <div class="timeline">

    <section class="p0">
      <div class="phase-header">
        <div class="phase-dot">0</div>
        <div class="phase-title-wrap">
          <div class="phase-label">// Phase Zero ¬∑ Months 0‚Äì3</div>
          <div class="phase-title">Mathematical Bedrock</div>
        </div>
      </div>

      <div class="milestone">
        <div class="milestone-icon">üéØ</div>
        <div>
          <div class="milestone-title">Goal</div>
          <div class="milestone-text">Build the mathematical language that every architecture, loss function, and optimization algorithm is written in. No shortcuts here ‚Äî everything later depends on this.</div>
        </div>
      </div>

      <div class="cards-grid">
        <div class="card">
          <span class="card-icon">üî¢</span>
          <div class="card-title">Linear Algebra</div>
          <p>Vector spaces, matrix operations, eigendecomposition, SVD, PCA. Understand why word embeddings are just PCA on co-occurrence matrices.</p>
          <div class="tag-list">
            <span class="tag">SVD</span><span class="tag">PCA</span><span class="tag">Eigenvalues</span><span class="tag">Tensors</span>
          </div>
        </div>
        <div class="card">
          <span class="card-icon">üìê</span>
          <div class="card-title">Calculus & Optimization</div>
          <p>Multivariable derivatives, Jacobians, Hessians, chain rule. Non-convex optimization, SGD, Adam, momentum. Foundation of backpropagation.</p>
          <div class="tag-list">
            <span class="tag">Backprop</span><span class="tag">Adam</span><span class="tag">SGD</span><span class="tag">Chain Rule</span>
          </div>
        </div>
        <div class="card">
          <span class="card-icon">üé≤</span>
          <div class="card-title">Probability & Statistics</div>
          <p>Bayesian inference, Gaussian processes, latent variable models, MLE, MAP. This is the language of generative models, VAEs, and diffusion.</p>
          <div class="tag-list">
            <span class="tag">Bayes</span><span class="tag">MLE</span><span class="tag">Distributions</span><span class="tag">CRFs</span>
          </div>
        </div>
        <div class="card">
          <span class="card-icon">üì°</span>
          <div class="card-title">Information Theory</div>
          <p>Entropy, KL divergence, mutual information, cross-entropy loss. Essential for understanding tokenization efficiency, RLHF reward modeling.</p>
          <div class="tag-list">
            <span class="tag">KL Divergence</span><span class="tag">Cross-Entropy</span><span class="tag">Mutual Info</span>
          </div>
        </div>
      </div>

      <div class="table-card">
        <div class="table-card-header">
          <div class="dot-indicator"></div>
          <span>Phase 0 ‚Äî Recommended Resources</span>
        </div>
        <table>
          <tr><th>Resource</th><th>Focus</th><th>Format</th></tr>
          <tr><td>Gilbert Strang ‚Äî Linear Algebra (MIT OCW)</td><td>Full linear algebra foundation</td><td>Video + Textbook</td></tr>
          <tr><td>3Blue1Brown ‚Äî Essence of Linear Algebra</td><td>Geometric intuition for all concepts</td><td>Video Series</td></tr>
          <tr><td>Deep Learning Book (Goodfellow) Ch. 2‚Äì4</td><td>Math of ML: LA, Probability, Numerics</td><td>Free Online</td></tr>
          <tr><td>Probabilistic ML (Kevin Murphy, 2022)</td><td>Bayesian perspective on modern ML</td><td>Textbook</td></tr>
          <tr><td>CS229 Lecture Notes (Stanford)</td><td>Statistical learning theory</td><td>PDF Notes</td></tr>
          <tr><td>Khan Academy / Paul's Math Notes</td><td>Multivariable calculus refresher</td><td>Online</td></tr>
        </table>
      </div>

      <div class="card card-full">
        <div class="card-title">‚è± 3-Month Sprint Schedule</div>
        <div class="sprint-inner" style="display:grid;grid-template-columns:1fr 1fr;gap:16px">
          <div>
            <div class="timeline-block"><span class="tl-month">M1 W1‚Äì2</span><span class="tl-text">Linear Algebra: vectors, matrices, transformations, SVD, PCA ‚Äî Strang lectures + exercises</span></div>
            <div class="timeline-block"><span class="tl-month">M1 W3‚Äì4</span><span class="tl-text">Eigendecomposition, tensor basics, group theory intro for geometric deep learning</span></div>
            <div class="timeline-block"><span class="tl-month">M2 W1‚Äì2</span><span class="tl-text">Multivariable calculus: gradients, Jacobians, Hessians. Derive backprop manually.</span></div>
          </div>
          <div>
            <div class="timeline-block"><span class="tl-month">M2 W3‚Äì4</span><span class="tl-text">Probability &amp; statistics: distributions, Bayes, Gaussian processes, MLE/MAP</span></div>
            <div class="timeline-block"><span class="tl-month">M3 W1‚Äì2</span><span class="tl-text">Information theory: entropy, KL divergence, cross-entropy. Implement soft targets, label smoothing.</span></div>
            <div class="timeline-block"><span class="tl-month">M3 W3‚Äì4</span><span class="tl-text">Optimization theory: SGD, Adam, learning rate schedules, non-convex landscapes. Read original Adam paper.</span></div>
          </div>
        </div>
      </div>

      <div style="margin-top:16px">
        <div class="progress-wrap">
          <div class="progress-label"><span>Phase Progress</span><span>100%</span></div>
          <div class="progress-bar"><div class="progress-fill" style="width:100%"></div></div>
        </div>
      </div>
    </section>

    <!-- ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê -->
    <!-- PHASE 1 ‚Äî NLP THEORY + DEEP LEARNING -->
    <!-- ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê -->
    <section class="p1">
      <div class="phase-header">
        <div class="phase-dot">1</div>
        <div class="phase-title-wrap">
          <div class="phase-label">// Phase One ¬∑ Months 4‚Äì9</div>
          <div class="phase-title">NLP Theory + Deep Learning Core</div>
        </div>
      </div>

      <div class="milestone">
        <div class="milestone-icon">üéØ</div>
        <div>
          <div class="milestone-title">Goal</div>
          <div class="milestone-text">Master the Transformer architecture from first principles. Implement every component from scratch. Understand LLMs, BERT, GPT, fine-tuning, alignment, and reasoning methods at PhD depth.</div>
        </div>
      </div>

      <div class="cards-grid">
        <div class="card">
          <span class="card-icon">üìù</span>
          <div class="card-title">Word Representations</div>
          <p>One-hot ‚Üí Word2Vec ‚Üí GloVe ‚Üí FastText. Understand distributional semantics, the matrix factorization perspective, and why context = meaning.</p>
          <div class="tag-list"><span class="tag">Word2Vec</span><span class="tag">GloVe</span><span class="tag">Embeddings</span></div>
        </div>
        <div class="card">
          <span class="card-icon">üîÅ</span>
          <div class="card-title">Sequence Models</div>
          <p>RNNs, LSTMs, GRUs ‚Äî understand vanishing gradients, gating, and why they were superseded. Study HMMs and CRFs for structured prediction.</p>
          <div class="tag-list"><span class="tag">LSTM</span><span class="tag">GRU</span><span class="tag">HMM</span><span class="tag">CRF</span></div>
        </div>
        <div class="card">
          <span class="card-icon">‚ö°</span>
          <div class="card-title">The Transformer</div>
          <p>Scaled dot-product attention, multi-head attention, positional encoding, RoPE, layer norm, feed-forward blocks. Implement from scratch in PyTorch.</p>
          <div class="tag-list"><span class="tag">Attention</span><span class="tag">RoPE</span><span class="tag">LayerNorm</span><span class="tag">MHA</span></div>
        </div>
        <div class="card">
          <span class="card-icon">üß†</span>
          <div class="card-title">Pre-training Paradigms</div>
          <p>BERT (MLM + NSP), GPT (causal LM), T5 (span corruption). Understand why scale changes the game ‚Äî emergent capabilities, few-shot learning.</p>
          <div class="tag-list"><span class="tag">BERT</span><span class="tag">GPT</span><span class="tag">T5</span><span class="tag">Scaling</span></div>
        </div>
        <div class="card">
          <span class="card-icon">üéì</span>
          <div class="card-title">Alignment &amp; RLHF</div>
          <p>Instruction tuning, SFT, PPO-based RLHF, DPO. Understand why reward models fail and what Constitutional AI solves. Read InstructGPT paper carefully.</p>
          <div class="tag-list"><span class="tag">DPO</span><span class="tag">RLHF</span><span class="tag">SFT</span><span class="tag">PPO</span></div>
        </div>
        <div class="card">
          <span class="card-icon">üîó</span>
          <div class="card-title">Efficient Fine-Tuning</div>
          <p>LoRA, QLoRA, Adapters, Prefix Tuning. Understand low-rank decomposition, why it works, and when full fine-tuning is still needed.</p>
          <div class="tag-list"><span class="tag">LoRA</span><span class="tag">QLoRA</span><span class="tag">PEFT</span><span class="tag">Adapters</span></div>
        </div>
      </div>

      <div class="table-card">
        <div class="table-card-header">
          <div class="dot-indicator"></div>
          <span>Core Papers ‚Äî Phase 1 Reading List</span>
        </div>
        <table>
          <tr><th>Year</th><th>Paper</th><th>Why It Matters</th></tr>
          <tr><td>2013</td><td>Word2Vec (Mikolov et al.)</td><td>Distributional representations; links to SVD/PCA</td></tr>
          <tr><td>2017</td><td>Attention Is All You Need</td><td>The backbone of modern AI ‚Äî mandatory reading</td></tr>
          <tr><td>2018</td><td>BERT (Devlin et al.)</td><td>Established the pre-train / fine-tune paradigm</td></tr>
          <tr><td>2020</td><td>GPT-3 (Brown et al.)</td><td>In-context learning; emergent scale effects</td></tr>
          <tr><td>2021</td><td>LoRA (Hu et al.)</td><td>The universal PEFT standard</td></tr>
          <tr><td>2022</td><td>Chain-of-Thought Prompting (Wei et al.)</td><td>Foundation of LLM reasoning capabilities</td></tr>
          <tr><td>2022</td><td>InstructGPT (Ouyang et al.)</td><td>RLHF for alignment ‚Äî landmark paper</td></tr>
          <tr><td>2023</td><td>DPO (Rafailov et al.)</td><td>Modern alignment standard without RL</td></tr>
          <tr><td>2023</td><td>LLaMA 1 &amp; 2 (Meta)</td><td>Open-source SOTA; study the architecture choices</td></tr>
          <tr><td>2024</td><td>Llama 3 / Qwen 2 Technical Reports</td><td>State-of-the-art open model training pipelines</td></tr>
        </table>
      </div>

      <div class="cards-grid">
        <div class="card">
          <div class="card-title">üõ† Practical Projects ‚Äî Phase 1</div>
          <ul class="checklist">
            <li><span class="check-box"></span>Implement Word2Vec skip-gram from scratch in NumPy</li>
            <li><span class="check-box"></span>Build a Transformer encoder/decoder in PyTorch, no libraries</li>
            <li><span class="check-box"></span>Fine-tune BERT on a classification task (SQuAD or GLUE)</li>
            <li><span class="check-box"></span>Implement a mini GPT-2 and train on small text corpus</li>
            <li><span class="check-box"></span>Implement LoRA fine-tuning on LLaMA using Hugging Face PEFT</li>
            <li><span class="check-box"></span>Train a simple reward model and run PPO-based RLHF loop</li>
          </ul>
        </div>
        <div class="card">
          <div class="card-title">üìö Courses</div>
          <ul class="checklist">
            <li><span class="check-box"></span><strong>Stanford CS224N</strong> ‚Äî NLP with Deep Learning (primary)</li>
            <li><span class="check-box"></span><strong>Stanford CME295</strong> ‚Äî Transformers &amp; LLMs (supplement)</li>
            <li><span class="check-box"></span><strong>fast.ai Part 1</strong> ‚Äî Practical Deep Learning (code-first grounding)</li>
            <li><span class="check-box"></span><strong>Andrej Karpathy</strong> ‚Äî Neural Nets: Zero to Hero (YouTube)</li>
            <li><span class="check-box"></span><strong>CMU 11-711</strong> ‚Äî Advanced NLP (PhD-level supplement)</li>
          </ul>
        </div>
      </div>

      <div style="margin-top:16px">
        <div class="progress-wrap">
          <div class="progress-label"><span>Phase Progress</span><span>80%</span></div>
          <div class="progress-bar"><div class="progress-fill" style="width:80%"></div></div>
        </div>
      </div>
    </section>

    <!-- ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê -->
    <!-- PHASE 2 ‚Äî COMPUTER VISION -->
    <!-- ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê -->
    <section class="p2">
      <div class="phase-header">
        <div class="phase-dot">2</div>
        <div class="phase-title-wrap">
          <div class="phase-label">// Phase Two ¬∑ Months 7‚Äì12 (parallel with Phase 1)</div>
          <div class="phase-title">Computer Vision + Multimodal AI</div>
        </div>
      </div>

      <div class="milestone">
        <div class="milestone-icon">üéØ</div>
        <div>
          <div class="milestone-title">Goal</div>
          <div class="milestone-text">Achieve PhD-level vision. Understand classical image processing, deep CNNs, ViT, diffusion models, 3D reconstruction (NeRF, Gaussian Splatting), and multimodal systems like CLIP and GPT-4o.</div>
        </div>
      </div>

      <div class="cards-grid">
        <div class="card">
          <span class="card-icon">üñº</span>
          <div class="card-title">Classical Vision &amp; Signals</div>
          <p>Images as signals. Linear filters, Fourier transforms, image pyramids, edge detection, SIFT, HOG. Physics of image formation, pinhole cameras.</p>
          <div class="tag-list"><span class="tag">Fourier</span><span class="tag">Filters</span><span class="tag">SIFT</span><span class="tag">Camera Model</span></div>
        </div>
        <div class="card">
          <span class="card-icon">üèó</span>
          <div class="card-title">CNNs &amp; Deep Architectures</div>
          <p>AlexNet ‚Üí VGG ‚Üí ResNet (residual connections) ‚Üí EfficientNet ‚Üí ConvNeXt. Understand local receptive fields, weight sharing, pooling strategies.</p>
          <div class="tag-list"><span class="tag">ResNet</span><span class="tag">ConvNeXt</span><span class="tag">Batch Norm</span></div>
        </div>
        <div class="card">
          <span class="card-icon">üëÅ</span>
          <div class="card-title">Vision Transformers</div>
          <p>ViT: image patches as tokens. DINO, DINOv2 self-supervised pretraining. Swin Transformer hierarchical representations. MAE masked autoencoding.</p>
          <div class="tag-list"><span class="tag">ViT</span><span class="tag">DINO</span><span class="tag">MAE</span><span class="tag">Swin</span></div>
        </div>
        <div class="card">
          <span class="card-icon">üåê</span>
          <div class="card-title">Multimodal: CLIP &amp; Beyond</div>
          <p>Contrastive Language-Image Pre-training. Zero-shot classification, image-text retrieval, visual question answering. BLIP, BLIP-2, LLaVA, GPT-4V.</p>
          <div class="tag-list"><span class="tag">CLIP</span><span class="tag">LLaVA</span><span class="tag">VQA</span><span class="tag">Contrastive</span></div>
        </div>
        <div class="card">
          <span class="card-icon">üåÄ</span>
          <div class="card-title">Diffusion Models</div>
          <p>DDPM, DDIM, score matching, classifier-free guidance. Latent diffusion (Stable Diffusion). DiT (Diffusion Transformers) as backbone of Sora 2.</p>
          <div class="tag-list"><span class="tag">DDPM</span><span class="tag">LDM</span><span class="tag">DiT</span><span class="tag">CFG</span></div>
        </div>
        <div class="card">
          <span class="card-icon">üßä</span>
          <div class="card-title">3D Vision &amp; Scene Reconstruction</div>
          <p>NeRF, Instant-NGP, 3D Gaussian Splatting. Projective geometry, fundamental matrix, eight-point algorithm, structure-from-motion (SfM).</p>
          <div class="tag-list"><span class="tag">NeRF</span><span class="tag">Gaussian Splatting</span><span class="tag">SfM</span><span class="tag">MVS</span></div>
        </div>
      </div>

      <div class="table-card">
        <div class="table-card-header">
          <div class="dot-indicator"></div>
          <span>Core Papers ‚Äî Phase 2 Reading List</span>
        </div>
        <table>
          <tr><th>Year</th><th>Paper</th><th>Why It Matters</th></tr>
          <tr><td>2012</td><td>AlexNet (Krizhevsky et al.)</td><td>Sparked the modern deep learning era</td></tr>
          <tr><td>2016</td><td>Deep Residual Networks ‚Äî ResNet (He et al.)</td><td>Residual connections enabled very deep networks</td></tr>
          <tr><td>2020</td><td>ViT ‚Äî An Image is Worth 16x16 Words</td><td>Transformer paradigm enters vision</td></tr>
          <tr><td>2020</td><td>NeRF (Mildenhall et al.)</td><td>Revolutionary neural 3D reconstruction</td></tr>
          <tr><td>2020</td><td>DDPM (Ho et al.)</td><td>Theoretical basis for modern image generation</td></tr>
          <tr><td>2021</td><td>CLIP (Radford et al.)</td><td>Foundation of modern vision-language models</td></tr>
          <tr><td>2022</td><td>MAE (He et al.)</td><td>Self-supervised ViT training</td></tr>
          <tr><td>2022</td><td>LDM / Stable Diffusion (Rombach et al.)</td><td>Practical high-res diffusion generation</td></tr>
          <tr><td>2023</td><td>3D Gaussian Splatting</td><td>Real-time neural 3D rendering</td></tr>
          <tr><td>2024</td><td>VGGT ‚Äî Visual Geometry Grounded Transformer</td><td>Current frontier of 3D scene understanding</td></tr>
        </table>
      </div>

      <div class="cards-grid">
        <div class="card">
          <div class="card-title">üõ† Practical Projects ‚Äî Phase 2</div>
          <ul class="checklist">
            <li><span class="check-box"></span>Implement a CNN from scratch; train on CIFAR-10</li>
            <li><span class="check-box"></span>Fine-tune ViT on a custom image dataset</li>
            <li><span class="check-box"></span>Reproduce CLIP contrastive training on a small dataset</li>
            <li><span class="check-box"></span>Implement DDPM from scratch; generate MNIST samples</li>
            <li><span class="check-box"></span>Train a tiny NeRF on a set of images (using tiny-nerf)</li>
            <li><span class="check-box"></span>Fine-tune LLaVA or InternVL for a visual QA task</li>
          </ul>
        </div>
        <div class="card">
          <div class="card-title">üìö Courses</div>
          <ul class="checklist">
            <li><span class="check-box"></span><strong>Stanford CS231N</strong> ‚Äî CNNs for Visual Recognition (primary)</li>
            <li><span class="check-box"></span><strong>MIT 6.869</strong> ‚Äî Advances in Computer Vision</li>
            <li><span class="check-box"></span><strong>CMU 16-825</strong> ‚Äî Learning for 3D Vision</li>
            <li><span class="check-box"></span><strong>CVPR / ECCV tutorials</strong> ‚Äî Diffusion models, NeRF workshops</li>
            <li><span class="check-box"></span><strong>Justin Johnson ‚Äî EECS 498</strong> (Michigan, free videos)</li>
          </ul>
        </div>
      </div>

      <div style="margin-top:16px">
        <div class="progress-wrap">
          <div class="progress-label"><span>Phase Progress</span><span>70%</span></div>
          <div class="progress-bar"><div class="progress-fill" style="width:70%"></div></div>
        </div>
      </div>
    </section>

    <!-- ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê -->
    <!-- PHASE 3 ‚Äî SYSTEMS & INFRASTRUCTURE -->
    <!-- ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê -->
    <section class="p3">
      <div class="phase-header">
        <div class="phase-dot">3</div>
        <div class="phase-title-wrap">
          <div class="phase-label">// Phase Three ¬∑ Months 12‚Äì20</div>
          <div class="phase-title">Systems, Infrastructure &amp; Scale</div>
        </div>
      </div>

      <div class="milestone">
        <div class="milestone-icon">üéØ</div>
        <div>
          <div class="milestone-title">Goal</div>
          <div class="milestone-text">Become the engineer who can actually train models. Understand GPU memory, custom CUDA/Triton kernels, distributed training, tokenization, data curation, and scaling laws at a production level.</div>
        </div>
      </div>

      <div class="cards-grid">
        <div class="card">
          <span class="card-icon">üíæ</span>
          <div class="card-title">GPU Architecture &amp; Memory</div>
          <p>CUDA programming model: warps, SMs, shared memory. The HBM ‚Üí SRAM hierarchy. Understand memory bandwidth bottlenecks that define training speed.</p>
          <div class="tag-list"><span class="tag">CUDA</span><span class="tag">SRAM</span><span class="tag">HBM</span><span class="tag">Warps</span></div>
        </div>
        <div class="card">
          <span class="card-icon">‚ö°</span>
          <div class="card-title">Efficient Attention</div>
          <p>FlashAttention and FlashAttention2: IO-aware attention that avoids quadratic memory. Write Triton kernels. Understand tiling, online softmax, backward pass.</p>
          <div class="tag-list"><span class="tag">FlashAttn2</span><span class="tag">Triton</span><span class="tag">Tiling</span><span class="tag">IO-Aware</span></div>
        </div>
        <div class="card">
          <span class="card-icon">üåê</span>
          <div class="card-title">Distributed Training</div>
          <p>Data parallelism (DDP), tensor parallelism (Megatron), pipeline parallelism (GPipe), ZeRO (DeepSpeed). 3D parallelism for trillion-parameter models.</p>
          <div class="tag-list"><span class="tag">DDP</span><span class="tag">DeepSpeed</span><span class="tag">Megatron</span><span class="tag">ZeRO</span></div>
        </div>
        <div class="card">
          <span class="card-icon">üî§</span>
          <div class="card-title">Tokenization &amp; Data Engineering</div>
          <p>BPE, WordPiece, SentencePiece. Build a tokenizer from scratch. Data curation: Common Crawl filtering, deduplication, quality scoring, domain mixing.</p>
          <div class="tag-list"><span class="tag">BPE</span><span class="tag">SentencePiece</span><span class="tag">Dedup</span><span class="tag">FineWeb</span></div>
        </div>
        <div class="card">
          <span class="card-icon">üìä</span>
          <div class="card-title">Scaling Laws</div>
          <p>Chinchilla laws, compute-optimal training, emergent capabilities. Use scaling laws to forecast model quality before billion-dollar training runs.</p>
          <div class="tag-list"><span class="tag">Chinchilla</span><span class="tag">Compute-Optimal</span><span class="tag">IsoFLOP</span></div>
        </div>
        <div class="card">
          <span class="card-icon">üóú</span>
          <div class="card-title">Quantization &amp; Efficiency</div>
          <p>FP8, BF16 mixed precision. Post-training quantization: GPTQ, AWQ. QLoRA for efficient fine-tuning. Speculative decoding for faster inference.</p>
          <div class="tag-list"><span class="tag">GPTQ</span><span class="tag">AWQ</span><span class="tag">QLoRA</span><span class="tag">FP8</span></div>
        </div>
      </div>

      <div class="table-card">
        <div class="table-card-header">
          <div class="dot-indicator"></div>
          <span>Core Papers ‚Äî Phase 3 Systems Reading List</span>
        </div>
        <table>
          <tr><th>Year</th><th>Paper</th><th>Why It Matters</th></tr>
          <tr><td>2022</td><td>FlashAttention (Dao et al.)</td><td>IO-aware attention ‚Äî mandatory for training engineers</td></tr>
          <tr><td>2022</td><td>Chinchilla Scaling Laws (Hoffmann et al.)</td><td>How to allocate compute optimally</td></tr>
          <tr><td>2022</td><td>ZeRO / DeepSpeed (Rajbhandari et al.)</td><td>Trillion-parameter training on commodity GPUs</td></tr>
          <tr><td>2023</td><td>FlashAttention-2 (Dao et al.)</td><td>2√ó speedup; standard for all modern training</td></tr>
          <tr><td>2023</td><td>Megatron-LM (NVIDIA)</td><td>3D tensor parallelism for massive clusters</td></tr>
          <tr><td>2024</td><td>FineWeb (HuggingFace)</td><td>State-of-the-art web data curation pipeline</td></tr>
          <tr><td>2024</td><td>FlashAttention-3 (Dao et al.)</td><td>H100-native optimized attention kernels</td></tr>
          <tr><td>2025</td><td>Ultra-Scale Playbook (HuggingFace)</td><td>Comprehensive distributed training guide</td></tr>
        </table>
      </div>

      <div class="cards-grid">
        <div class="card">
          <div class="card-title">üõ† Practical Projects ‚Äî Phase 3</div>
          <ul class="checklist">
            <li><span class="check-box"></span>Build a BPE tokenizer from scratch ‚Äî train on 1GB text corpus</li>
            <li><span class="check-box"></span>Write a Triton kernel for FlashAttention forward pass</li>
            <li><span class="check-box"></span>Set up DDP training on a 4-GPU node; benchmark throughput</li>
            <li><span class="check-box"></span>Train a 1B-param LM from scratch (Stanford CS336 project)</li>
            <li><span class="check-box"></span>Reproduce FineWeb-style data curation pipeline</li>
            <li><span class="check-box"></span>Implement speculative decoding; measure latency gains</li>
            <li><span class="check-box"></span>Profile a training run with NSight; find the bottleneck</li>
          </ul>
        </div>
        <div class="card">
          <div class="card-title">üìö Courses &amp; Resources</div>
          <ul class="checklist">
            <li><span class="check-box"></span><strong>Stanford CS336</strong> ‚Äî Language Modeling from Scratch (primary)</li>
            <li><span class="check-box"></span><strong>CUDA Programming Guide</strong> (NVIDIA docs)</li>
            <li><span class="check-box"></span><strong>Triton documentation + tutorials</strong> (OpenAI / triton-lang)</li>
            <li><span class="check-box"></span><strong>HuggingFace Ultra-Scale Playbook</strong></li>
            <li><span class="check-box"></span><strong>Sasha Rush ‚Äî miniGPT / annotated transformer</strong></li>
          </ul>
        </div>
      </div>

      <div style="margin-top:16px">
        <div class="progress-wrap">
          <div class="progress-label"><span>Phase Progress</span><span>60%</span></div>
          <div class="progress-bar"><div class="progress-fill" style="width:60%"></div></div>
        </div>
      </div>
    </section>

    <!-- ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê -->
    <!-- PHASE 4 ‚Äî REASONING, AGENTS, RESEARCH -->
    <!-- ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê -->
    <section class="p4">
      <div class="phase-header">
        <div class="phase-dot">4</div>
        <div class="phase-title-wrap">
          <div class="phase-label">// Phase Four ¬∑ Months 20‚Äì30</div>
          <div class="phase-title">Reasoning, Agents &amp; Research Frontier</div>
        </div>
      </div>

      <div class="milestone">
        <div class="milestone-icon">üéØ</div>
        <div>
          <div class="milestone-title">Goal</div>
          <div class="milestone-text">Operate at the frontier. Understand DeepSeek-R1's pure RL training, GRPO, agentic frameworks, RAG, video world models, and contribute original research. Begin publishing.</div>
        </div>
      </div>

      <div class="cards-grid">
        <div class="card">
          <span class="card-icon">ü§î</span>
          <div class="card-title">Reasoning &amp; Chain-of-Thought</div>
          <p>CoT, self-consistency, tree-of-thought, process reward models (PRM) vs outcome reward models (ORM). The "Open-Book Paradox" ‚Äî why closed-book training builds stronger reasoners.</p>
          <div class="tag-list"><span class="tag">CoT</span><span class="tag">PRM</span><span class="tag">ORM</span><span class="tag">Self-Consistency</span></div>
        </div>
        <div class="card">
          <span class="card-icon">üèã</span>
          <div class="card-title">RL for Reasoning</div>
          <p>DeepSeek-R1: pure RL from base model without SFT. GRPO (Group Relative Policy Optimization). "Aha moment" emergence. STILL (Self-Play + RL).</p>
          <div class="tag-list"><span class="tag">GRPO</span><span class="tag">R1</span><span class="tag">STILL</span><span class="tag">Verifiable Rewards</span></div>
        </div>
        <div class="card">
          <span class="card-icon">ü§ñ</span>
          <div class="card-title">Agentic AI &amp; Tool Use</div>
          <p>ReAct, Toolformer, function calling APIs. Multi-agent orchestration. Software engineering agents (SWE-Bench, SWE-RL). Memory architectures for long-horizon tasks.</p>
          <div class="tag-list"><span class="tag">ReAct</span><span class="tag">SWE-Bench</span><span class="tag">Multi-Agent</span><span class="tag">MCP</span></div>
        </div>
        <div class="card">
          <span class="card-icon">üîç</span>
          <div class="card-title">RAG &amp; Knowledge Grounding</div>
          <p>Retrieval-Augmented Generation: dense retrieval, hybrid search, re-ranking. Advanced RAG: HyDE, RAPTOR, agentic RAG. Reducing hallucinations in production.</p>
          <div class="tag-list"><span class="tag">RAG</span><span class="tag">FAISS</span><span class="tag">HyDE</span><span class="tag">RAPTOR</span></div>
        </div>
        <div class="card">
          <span class="card-icon">üé¨</span>
          <div class="card-title">Video &amp; World Models</div>
          <p>Sora 2 / Sora 3 architecture (Diffusion Transformer). VideoThinkBench: video generation models for spatial reasoning. Runway Gen-4.5 physics simulation.</p>
          <div class="tag-list"><span class="tag">DiT</span><span class="tag">Sora</span><span class="tag">VideoThink</span><span class="tag">World Models</span></div>
        </div>
        <div class="card">
          <span class="card-icon">üî¨</span>
          <div class="card-title">Interpretability &amp; Safety</div>
          <p>Mechanistic interpretability: circuits, superposition, feature visualization. Anthropic's sparse autoencoders. Constitutional AI, Debate, scalable oversight methods.</p>
          <div class="tag-list"><span class="tag">Circuits</span><span class="tag">SAE</span><span class="tag">CAI</span><span class="tag">Oversight</span></div>
        </div>
      </div>

      <div class="table-card">
        <div class="table-card-header">
          <div class="dot-indicator"></div>
          <span>Core Papers ‚Äî Frontier Research (2023‚Äì2026)</span>
        </div>
        <table>
          <tr><th>Year</th><th>Paper</th><th>Significance</th></tr>
          <tr><td>2023</td><td>Mamba: Linear-Time Sequence Modeling</td><td>State space alternative to attention ‚Äî potential successor</td></tr>
          <tr><td>2023</td><td>Toolformer (Schick et al.)</td><td>Models teaching themselves to use APIs</td></tr>
          <tr><td>2024</td><td>DeepSeek-R1 (DeepSeek-AI)</td><td>Pure RL-trained reasoner ‚Äî new training paradigm</td></tr>
          <tr><td>2024</td><td>GPT-4o Technical Report (OpenAI)</td><td>Omni-modal native multimodality at scale</td></tr>
          <tr><td>2024</td><td>SWE-Bench Verified</td><td>Gold standard for software engineering agents</td></tr>
          <tr><td>2025</td><td>DeepSeek-V3 (MoE, open-source)</td><td>Open SOTA ‚Äî study training efficiency innovations</td></tr>
          <tr><td>2025</td><td>SWE-RL (Software Engineering RL)</td><td>RL for coding; open-book vs closed-book insight</td></tr>
          <tr><td>2025</td><td>VideoThinkBench</td><td>Video generation surpassing VLMs in spatial reasoning</td></tr>
          <tr><td>2025</td><td>Lumina-Video (V2A synthesis)</td><td>Synchronized video-to-audio generation</td></tr>
          <tr><td>2026</td><td>VGGT + frontier geometry papers</td><td>Next-gen 3D-aware visual reasoning</td></tr>
        </table>
      </div>

      <div class="cards-grid">
        <div class="card">
          <div class="card-title">üõ† Practical Projects ‚Äî Phase 4</div>
          <ul class="checklist">
            <li><span class="check-box"></span>Implement GRPO from scratch; train a math reasoning model</li>
            <li><span class="check-box"></span>Build a multi-step ReAct agent with tool use (search, code exec)</li>
            <li><span class="check-box"></span>Reproduce DeepSeek-R1 RL training on a small task</li>
            <li><span class="check-box"></span>Build an advanced RAG pipeline; benchmark vs naive RAG</li>
            <li><span class="check-box"></span>Run mechanistic interpretability on a 1B model (TransformerLens)</li>
            <li><span class="check-box"></span>Submit to a SWE-Bench leaderboard</li>
            <li><span class="check-box"></span><strong style="color:var(--accent5)">Write and submit a research paper</strong></li>
          </ul>
        </div>
        <div class="card">
          <div class="card-title">üìö Courses &amp; Seminars</div>
          <ul class="checklist">
            <li><span class="check-box"></span><strong>Stanford CME295</strong> ‚Äî Transformers &amp; LLMs (advanced)</li>
            <li><span class="check-box"></span><strong>Berkeley CS285</strong> ‚Äî Deep Reinforcement Learning</li>
            <li><span class="check-box"></span><strong>CMU 11-713</strong> ‚Äî Advanced NLP (PhD seminar)</li>
            <li><span class="check-box"></span><strong>BAIR / CSAIL reading groups</strong> (join or follow online)</li>
            <li><span class="check-box"></span><strong>NeurIPS / ICML / CVPR</strong> ‚Äî Attend tutorials, workshops</li>
          </ul>
        </div>
      </div>

      <div style="margin-top:16px">
        <div class="progress-wrap">
          <div class="progress-label"><span>Phase Progress</span><span>40%</span></div>
          <div class="progress-bar"><div class="progress-fill" style="width:40%"></div></div>
        </div>
      </div>
    </section>

  </div><!-- /timeline -->

  <!-- SENIOR ENGINEER SKILLS -->
  <div class="section-divider">
    <div class="section-divider-line"></div>
    <div class="section-divider-text">// Senior AI Engineer Skills</div>
    <div class="section-divider-line"></div>
  </div>

  <!-- FIX: removed incorrect "p2" phase class; using "skills-section" instead -->
  <div style="margin-bottom:16px" class="skills-section">
    <div class="cards-grid">
      <div class="card">
        <span class="card-icon">üêç</span>
        <div class="card-title">Python &amp; ML Stack</div>
        <p>PyTorch (primary), JAX, Triton. Hugging Face Transformers, Datasets, PEFT. Weights &amp; Biases, MLflow for experiment tracking. High-performance Python profiling.</p>
        <div class="tag-list">
          <span class="tag">PyTorch</span><span class="tag">JAX</span><span class="tag">HuggingFace</span><span class="tag">W&amp;B</span>
        </div>
      </div>
      <div class="card">
        <span class="card-icon">‚òÅÔ∏è</span>
        <div class="card-title">Cloud &amp; Infra</div>
        <p>AWS / GCP / Azure GPU instances. Kubernetes for ML workloads. Docker containerization. Terraform infrastructure-as-code. CI/CD for model pipelines.</p>
        <div class="tag-list">
          <span class="tag">AWS</span><span class="tag">Docker</span><span class="tag">K8s</span><span class="tag">CI/CD</span>
        </div>
      </div>
      <div class="card">
        <span class="card-icon">üöÄ</span>
        <div class="card-title">Model Serving &amp; Deployment</div>
        <p>vLLM, TGI (Text Generation Inference), Ollama. ONNX export, TensorRT optimization. Batching strategies, KV cache management, SLA-aware inference.</p>
        <div class="tag-list">
          <span class="tag">vLLM</span><span class="tag">TGI</span><span class="tag">TensorRT</span><span class="tag">KV Cache</span>
        </div>
      </div>
      <div class="card">
        <span class="card-icon">üìè</span>
        <div class="card-title">Evaluation &amp; Benchmarking</div>
        <p>MMLU, HumanEval, MATH, MT-Bench. LLM-as-a-judge frameworks. Red-teaming and safety evaluation. Building custom evals for production use cases.</p>
        <div class="tag-list">
          <span class="tag">MMLU</span><span class="tag">LM-Eval</span><span class="tag">LLM-Judge</span><span class="tag">Red-Teaming</span>
        </div>
      </div>
    </div>
  </div>

  <!-- MASTER TIMELINE -->
  <div class="section-divider">
    <div class="section-divider-line"></div>
    <div class="section-divider-text">// Master Timeline</div>
    <div class="section-divider-line"></div>
  </div>

  <div class="table-card" style="margin-bottom:24px">
    <div class="table-card-header">
      <!-- FIX: use CSS var instead of inline hex for dot-indicator -->
      <div class="dot-indicator" style="background:var(--accent1);box-shadow:0 0 8px var(--accent1)"></div>
      <span>36-Month Fast-Track Plan</span>
    </div>
    <table>
      <tr><th>Period</th><th>Phase</th><th>Primary Focus</th><th>Milestone</th></tr>
      <tr><td>M0‚Äì3</td><td>Phase 0</td><td>Math: LinAlg, Calc, Prob, InfoTheory</td><td>Pass ML foundations exam; derive backprop from scratch</td></tr>
      <tr><td>M3‚Äì9</td><td>Phase 1a</td><td>NLP: Transformers, LLMs, Alignment (CS224N)</td><td>Implement Transformer from scratch; fine-tune LLaMA</td></tr>
      <tr><td>M6‚Äì12</td><td>Phase 1b</td><td>Vision: CNNs, ViT, Diffusion, NeRF (CS231N)</td><td>Train diffusion model; implement ViT; reproduce CLIP</td></tr>
      <tr><td>M12‚Äì18</td><td>Phase 2</td><td>Systems: GPU, FlashAttn, Distributed (CS336)</td><td>Train 1B model from scratch; write Triton kernel</td></tr>
      <tr><td>M16‚Äì22</td><td>Phase 3a</td><td>Reasoning: CoT, RLHF, DPO, GRPO</td><td>Implement DPO pipeline; reproduce DeepSeek-R1 on small task</td></tr>
      <tr><td>M20‚Äì28</td><td>Phase 3b</td><td>Agents: ReAct, RAG, Tool Use, Safety</td><td>SWE-Bench submission; build production RAG system</td></tr>
      <tr><td>M24‚Äì36</td><td>Phase 4</td><td>Research Frontier + Original Contribution</td><td>First paper accepted at top-tier conference (NeurIPS/CVPR/ICLR)</td></tr>
    </table>
  </div>

  <!-- OUTCOMES -->
  <div class="section-divider">
    <div class="section-divider-line"></div>
    <div class="section-divider-text">// End State</div>
    <div class="section-divider-line"></div>
  </div>

  <div class="outcome-grid">
    <div class="outcome-card oc1">
      <div class="outcome-icon">üéì</div>
      <div class="outcome-title">PhD-Level Theory</div>
      <div class="outcome-text">Can pass qualifying exams at CMU, MIT, Stanford. Fluent in the full literature from HMMs to Mamba. Capable of reading and critiquing any new paper at submission.</div>
    </div>
    <div class="outcome-card oc2">
      <div class="outcome-icon">‚öôÔ∏è</div>
      <div class="outcome-title">Senior Engineer</div>
      <div class="outcome-text">Trains and deploys production models. Writes custom GPU kernels. Builds agentic pipelines, RAG systems, and evaluation frameworks. Proficient in distributed training at scale.</div>
    </div>
    <div class="outcome-card oc3">
      <div class="outcome-icon">üî¨</div>
      <div class="outcome-title">Researcher</div>
      <div class="outcome-text">Publishes at CVPR, NeurIPS, ICLR, or ACL. Contributes to open-source frontier models. Identifies and solves novel problems at the intersection of NLP, vision, and reasoning.</div>
    </div>
  </div>

  <!-- KEY INSIGHT BOX ‚Äî FIX: replaced all inline styles with .insight-box class -->
  <div class="insight-box">
    <div class="insight-label">// Key Insight: The Fast-Track Strategy</div>
    <div class="insight-text">
      Phases 1 and 2 (NLP + Vision) overlap intentionally ‚Äî both rely on the same Transformer backbone. Once you deeply understand one, the other accelerates. The most common trap is spending too long on theory before shipping projects. <span class="highlight">Ship something every two weeks.</span> The research frontier is won by those who can both read the paper and run the experiment.
    </div>
  </div>

  <footer>
    <p>AI/ML RESEARCHER ROADMAP // PHASED EXECUTION PLAN // 2025‚Äì2028</p>
    <p style="margin-top:8px;opacity:0.5">Based on Stanford CS224N ¬∑ CS231N ¬∑ CS336 ¬∑ CME295 ¬∑ MIT 6.869 ¬∑ CMU 11-713 ¬∑ Berkeley CS285</p>
  </footer>

</div>

<script>
  const html = document.documentElement;
  const btn = document.getElementById('themeToggle');
  const icon = document.getElementById('toggleIcon');
  const label = document.getElementById('toggleLabel');

  // Restore saved preference
  const saved = localStorage.getItem('theme') || 'dark';
  setTheme(saved);

  btn.addEventListener('click', () => {
    const current = html.getAttribute('data-theme');
    setTheme(current === 'dark' ? 'light' : 'dark');
  });

  function setTheme(t) {
    html.setAttribute('data-theme', t);
    localStorage.setItem('theme', t);
    if (t === 'light') {
      icon.textContent = 'üåô';
      label.textContent = 'Dark';
    } else {
      icon.textContent = '‚òÄÔ∏è';
      label.textContent = 'Light';
    }
  }
</script>

</body>
</html>
