<!DOCTYPE html>
<html lang="en" data-theme="dark">
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<title>AI/ML PhD + Senior Engineer Roadmap</title>
<link rel="preconnect" href="https://fonts.googleapis.com">
<link href="https://fonts.googleapis.com/css2?family=Space+Mono:ital,wght@0,400;0,700;1,400&family=Syne:wght@400;600;700;800&family=DM+Mono:wght@300;400;500&display=swap" rel="stylesheet">
<link rel="stylesheet" href="styles.css">
</head>
<body>

<!-- ‚ïê‚ïê‚ïê‚ïê THEME TOGGLE ‚ïê‚ïê‚ïê‚ïê -->
<button class="theme-toggle" id="themeToggle" aria-label="Toggle theme">
  <span class="toggle-icon" id="toggleIcon">‚òÄÔ∏è</span>
  <span id="toggleLabel">Light</span>
</button>

<div class="container">

  <!-- HEADER -->
  <header>
    <div class="badge">// FAST-TRACK ROADMAP 2025‚Äì2028</div>
    <h1>AI/ML PhD Level<br>+ Senior Engineer</h1>
    <p class="subtitle">From mathematical foundations to the research frontier ‚Äî a complete, phased execution plan for mastering the full stack of modern artificial intelligence.</p>
    <div class="stats-row">
      <div class="stat"><span class="stat-val">5</span><span class="stat-label">Phases</span></div>
      <div class="stat"><span class="stat-val">~3yr</span><span class="stat-label">Fast Track</span></div>
      <div class="stat"><span class="stat-val">100+</span><span class="stat-label">Core Papers</span></div>
      <div class="stat"><span class="stat-val">PhD</span><span class="stat-label">Target Level</span></div>
    </div>
  </header>

  <!-- ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê -->
  <!-- PHASE 0 ‚Äî MATH BEDROCK -->
  <!-- ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê -->
  <div class="timeline">

    <section class="p0">
      <div class="phase-header">
        <div class="phase-dot">0</div>
        <div class="phase-title-wrap">
          <div class="phase-label">// Phase Zero ¬∑ Months 0‚Äì3</div>
          <div class="phase-title">Mathematical Bedrock</div>
        </div>
      </div>

      <div class="milestone">
        <div class="milestone-icon">üéØ</div>
        <div>
          <div class="milestone-title">Goal</div>
          <div class="milestone-text">Build the mathematical language that every architecture, loss function, and optimization algorithm is written in. No shortcuts here ‚Äî everything later depends on this.</div>
        </div>
      </div>

      <div class="cards-grid">
        <div class="card">
          <span class="card-icon">üî¢</span>
          <div class="card-title">Linear Algebra</div>
          <p>Vector spaces, matrix operations, eigendecomposition, SVD, PCA. Understand why word embeddings are just PCA on co-occurrence matrices.</p>
          <div class="tag-list">
            <span class="tag">SVD</span><span class="tag">PCA</span><span class="tag">Eigenvalues</span><span class="tag">Tensors</span>
          </div>
        </div>
        <div class="card">
          <span class="card-icon">üìê</span>
          <div class="card-title">Calculus & Optimization</div>
          <p>Multivariable derivatives, Jacobians, Hessians, chain rule. Non-convex optimization, SGD, Adam, momentum. Foundation of backpropagation.</p>
          <div class="tag-list">
            <span class="tag">Backprop</span><span class="tag">Adam</span><span class="tag">SGD</span><span class="tag">Chain Rule</span>
          </div>
        </div>
        <div class="card">
          <span class="card-icon">üé≤</span>
          <div class="card-title">Probability & Statistics</div>
          <p>Bayesian inference, Gaussian processes, latent variable models, MLE, MAP. This is the language of generative models, VAEs, and diffusion.</p>
          <div class="tag-list">
            <span class="tag">Bayes</span><span class="tag">MLE</span><span class="tag">Distributions</span><span class="tag">CRFs</span>
          </div>
        </div>
        <div class="card">
          <span class="card-icon">üì°</span>
          <div class="card-title">Information Theory</div>
          <p>Entropy, KL divergence, mutual information, cross-entropy loss. Essential for understanding tokenization efficiency, RLHF reward modeling.</p>
          <div class="tag-list">
            <span class="tag">KL Divergence</span><span class="tag">Cross-Entropy</span><span class="tag">Mutual Info</span>
          </div>
        </div>
      </div>

      <div class="table-card">
        <div class="table-card-header">
          <div class="dot-indicator"></div>
          <span>Phase 0 ‚Äî Recommended Resources</span>
        </div>
        <table>
          <tr><th>Resource</th><th>Focus</th><th>Format</th></tr>
          <tr><td><a href="https://ocw.mit.edu/courses/18-06-linear-algebra-spring-2010/" target="_blank" rel="noopener noreferrer">Gilbert Strang ‚Äî Linear Algebra (MIT OCW)</a></td><td>Full linear algebra foundation</td><td>Video + Textbook</td></tr>
          <tr><td><a href="https://www.youtube.com/playlist?list=PLZHQObOWTQDPD3MizzM2xVFitgF8hE_ab" target="_blank" rel="noopener noreferrer">3Blue1Brown ‚Äî Essence of Linear Algebra</a></td><td>Geometric intuition for all concepts</td><td>Video Series</td></tr>
          <tr><td><a href="https://www.deeplearningbook.org/" target="_blank" rel="noopener noreferrer">Deep Learning Book (Goodfellow) Ch. 2‚Äì4</a></td><td>Math of ML: LA, Probability, Numerics</td><td>Free Online</td></tr>
          <tr><td><a href="https://probml.github.io/pml-book/book1.html" target="_blank" rel="noopener noreferrer">Probabilistic ML (Kevin Murphy, 2022)</a></td><td>Bayesian perspective on modern ML</td><td>Textbook</td></tr>
          <tr><td><a href="https://cs229.stanford.edu/main_notes.pdf" target="_blank" rel="noopener noreferrer">CS229 Lecture Notes (Stanford)</a></td><td>Statistical learning theory</td><td>PDF Notes</td></tr>
          <tr><td>Khan Academy / Paul's Math Notes</td><td>Multivariable calculus refresher</td><td>Online</td></tr>
        </table>
      </div>

      <div class="card card-full">
        <div class="card-title">‚è± 3-Month Sprint Schedule</div>
        <div class="sprint-inner" style="display:grid;grid-template-columns:1fr 1fr;gap:16px">
          <div>
            <div class="timeline-block"><span class="tl-month">M1 W1‚Äì2</span><span class="tl-text">Linear Algebra: vectors, matrices, transformations, SVD, PCA ‚Äî Strang lectures + exercises</span></div>
            <div class="timeline-block"><span class="tl-month">M1 W3‚Äì4</span><span class="tl-text">Eigendecomposition, tensor basics, group theory intro for geometric deep learning</span></div>
            <div class="timeline-block"><span class="tl-month">M2 W1‚Äì2</span><span class="tl-text">Multivariable calculus: gradients, Jacobians, Hessians. Derive backprop manually.</span></div>
          </div>
          <div>
            <div class="timeline-block"><span class="tl-month">M2 W3‚Äì4</span><span class="tl-text">Probability &amp; statistics: distributions, Bayes, Gaussian processes, MLE/MAP</span></div>
            <div class="timeline-block"><span class="tl-month">M3 W1‚Äì2</span><span class="tl-text">Information theory: entropy, KL divergence, cross-entropy. Implement soft targets, label smoothing.</span></div>
            <div class="timeline-block"><span class="tl-month">M3 W3‚Äì4</span><span class="tl-text">Optimization theory: SGD, Adam, learning rate schedules, non-convex landscapes. Read original Adam paper.</span></div>
          </div>
        </div>
      </div>

      <div style="margin-top:16px">
        <div class="progress-wrap">
          <div class="progress-label"><span>Phase Progress</span><span>100%</span></div>
          <div class="progress-bar"><div class="progress-fill" style="width:100%"></div></div>
        </div>
      </div>
    </section>

    <!-- ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê -->
    <!-- PHASE 1 ‚Äî NLP THEORY + DEEP LEARNING -->
    <!-- ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê -->
    <section class="p1">
      <div class="phase-header">
        <div class="phase-dot">1</div>
        <div class="phase-title-wrap">
          <div class="phase-label">// Phase One ¬∑ Months 4‚Äì9</div>
          <div class="phase-title">NLP Theory + Deep Learning Core</div>
        </div>
      </div>

      <div class="milestone">
        <div class="milestone-icon">üéØ</div>
        <div>
          <div class="milestone-title">Goal</div>
          <div class="milestone-text">Master the Transformer architecture from first principles. Implement every component from scratch. Understand LLMs, BERT, GPT, fine-tuning, alignment, and reasoning methods at PhD depth.</div>
        </div>
      </div>

      <div class="cards-grid">
        <div class="card">
          <span class="card-icon">üìù</span>
          <div class="card-title">Word Representations</div>
          <p>One-hot ‚Üí Word2Vec ‚Üí GloVe ‚Üí FastText. Understand distributional semantics, the matrix factorization perspective, and why context = meaning.</p>
          <div class="tag-list"><span class="tag">Word2Vec</span><span class="tag">GloVe</span><span class="tag">Embeddings</span></div>
        </div>
        <div class="card">
          <span class="card-icon">üîÅ</span>
          <div class="card-title">Sequence Models</div>
          <p>RNNs, LSTMs, GRUs ‚Äî understand vanishing gradients, gating, and why they were superseded. Study HMMs and CRFs for structured prediction.</p>
          <div class="tag-list"><span class="tag">LSTM</span><span class="tag">GRU</span><span class="tag">HMM</span><span class="tag">CRF</span></div>
        </div>
        <div class="card">
          <span class="card-icon">‚ö°</span>
          <div class="card-title">The Transformer</div>
          <p>Scaled dot-product attention, multi-head attention, positional encoding, RoPE, layer norm, feed-forward blocks. Implement from scratch in PyTorch.</p>
          <div class="tag-list"><span class="tag">Attention</span><span class="tag">RoPE</span><span class="tag">LayerNorm</span><span class="tag">MHA</span></div>
        </div>
        <div class="card">
          <span class="card-icon">üß†</span>
          <div class="card-title">Pre-training Paradigms</div>
          <p>BERT (MLM + NSP), GPT (causal LM), T5 (span corruption). Understand why scale changes the game ‚Äî emergent capabilities, few-shot learning.</p>
          <div class="tag-list"><span class="tag">BERT</span><span class="tag">GPT</span><span class="tag">T5</span><span class="tag">Scaling</span></div>
        </div>
        <div class="card">
          <span class="card-icon">üéì</span>
          <div class="card-title">Alignment &amp; RLHF</div>
          <p>Instruction tuning, SFT, PPO-based RLHF, DPO. Understand why reward models fail and what Constitutional AI solves. Read InstructGPT paper carefully.</p>
          <div class="tag-list"><span class="tag">DPO</span><span class="tag">RLHF</span><span class="tag">SFT</span><span class="tag">PPO</span></div>
        </div>
        <div class="card">
          <span class="card-icon">üîó</span>
          <div class="card-title">Efficient Fine-Tuning</div>
          <p>LoRA, QLoRA, Adapters, Prefix Tuning. Understand low-rank decomposition, why it works, and when full fine-tuning is still needed.</p>
          <div class="tag-list"><span class="tag">LoRA</span><span class="tag">QLoRA</span><span class="tag">PEFT</span><span class="tag">Adapters</span></div>
        </div>
      </div>

      <div class="table-card">
        <div class="table-card-header">
          <div class="dot-indicator"></div>
          <span>Core Papers ‚Äî Phase 1 Reading List</span>
        </div>
        <table>
          <tr><th>Year</th><th>Paper</th><th>Why It Matters</th></tr>
          <tr><td>2013</td><td>Word2Vec (Mikolov et al.)</td><td>Distributional representations; links to SVD/PCA</td></tr>
          <tr><td>2017</td><td>Attention Is All You Need</td><td>The backbone of modern AI ‚Äî mandatory reading</td></tr>
          <tr><td>2018</td><td>BERT (Devlin et al.)</td><td>Established the pre-train / fine-tune paradigm</td></tr>
          <tr><td>2020</td><td>GPT-3 (Brown et al.)</td><td>In-context learning; emergent scale effects</td></tr>
          <tr><td>2021</td><td>LoRA (Hu et al.)</td><td>The universal PEFT standard</td></tr>
          <tr><td>2022</td><td>Chain-of-Thought Prompting (Wei et al.)</td><td>Foundation of LLM reasoning capabilities</td></tr>
          <tr><td>2022</td><td>InstructGPT (Ouyang et al.)</td><td>RLHF for alignment ‚Äî landmark paper</td></tr>
          <tr><td>2023</td><td>DPO (Rafailov et al.)</td><td>Modern alignment standard without RL</td></tr>
          <tr><td>2023</td><td>LLaMA 1 &amp; 2 (Meta)</td><td>Open-source SOTA; study the architecture choices</td></tr>
          <tr><td>2024</td><td>Llama 3 / Qwen 2 Technical Reports</td><td>State-of-the-art open model training pipelines</td></tr>
        </table>
      </div>

      <div class="cards-grid">
        <div class="card">
          <div class="card-title">üõ† Practical Projects ‚Äî Phase 1</div>
          <ul class="checklist">
            <li><span class="check-box"></span>Implement Word2Vec skip-gram from scratch in NumPy</li>
            <li><span class="check-box"></span>Build a Transformer encoder/decoder in PyTorch, no libraries</li>
            <li><span class="check-box"></span>Fine-tune BERT on a classification task (SQuAD or GLUE)</li>
            <li><span class="check-box"></span>Implement a mini GPT-2 and train on small text corpus</li>
            <li><span class="check-box"></span>Implement LoRA fine-tuning on LLaMA using Hugging Face PEFT</li>
            <li><span class="check-box"></span>Train a simple reward model and run PPO-based RLHF loop</li>
          </ul>
        </div>
        <div class="card">
          <div class="card-title">üìö Courses</div>
          <ul class="checklist">
            <li><span class="check-box"></span><strong><a href="https://web.stanford.edu/class/cs224n/" target="_blank" rel="noopener noreferrer">Stanford CS224N</a></strong> ‚Äî NLP with Deep Learning (primary)</li>
            <li><span class="check-box"></span><strong><a href="https://cme295.stanford.edu/" target="_blank" rel="noopener noreferrer">Stanford CME295</a></strong> ‚Äî Transformers &amp; LLMs (supplement)</li>
            <li><span class="check-box"></span><strong>fast.ai Part 1</strong> ‚Äî Practical Deep Learning (code-first grounding)</li>
            <li><span class="check-box"></span><strong>Andrej Karpathy</strong> ‚Äî Neural Nets: Zero to Hero (YouTube)</li>
            <li><span class="check-box"></span><strong><a href="http://www.cs.cmu.edu/~nasmith/ANLPS/" target="_blank" rel="noopener noreferrer">CMU 11-713</a></strong> ‚Äî Advanced NLP (PhD-level supplement)</li>
          </ul>
        </div>
      </div>

      <div style="margin-top:16px">
        <div class="progress-wrap">
          <div class="progress-label"><span>Phase Progress</span><span>80%</span></div>
          <div class="progress-bar"><div class="progress-fill" style="width:80%"></div></div>
        </div>
      </div>
    </section>

    <!-- ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê -->
    <!-- PHASE 2 ‚Äî COMPUTER VISION -->
    <!-- ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê -->
    <section class="p2">
      <div class="phase-header">
        <div class="phase-dot">2</div>
        <div class="phase-title-wrap">
          <div class="phase-label">// Phase Two ¬∑ Months 7‚Äì12 (parallel with Phase 1)</div>
          <div class="phase-title">Computer Vision + Multimodal AI</div>
        </div>
      </div>

      <div class="milestone">
        <div class="milestone-icon">üéØ</div>
        <div>
          <div class="milestone-title">Goal</div>
          <div class="milestone-text">Master visual representations from CNNs to Vision Transformers. Understand diffusion models, multimodal alignment (CLIP), and how vision and language converge at scale.</div>
        </div>
      </div>

      <div class="cards-grid">
        <div class="card">
          <span class="card-icon">üñºÔ∏è</span>
          <div class="card-title">CNNs &amp; Modern Architectures</div>
          <p>ResNet, EfficientNet, MobileNet. Understand inductive biases: translation equivariance, hierarchical features. Why CNNs dominated before Transformers.</p>
          <div class="tag-list"><span class="tag">ResNet</span><span class="tag">EfficientNet</span><span class="tag">Convolution</span></div>
        </div>
        <div class="card">
          <span class="card-icon">üëÅÔ∏è</span>
          <div class="card-title">Vision Transformers (ViT)</div>
          <p>Patch embeddings, positional encoding for 2D, scaling laws. Why ViT matches CNNs with pure Transformer architecture. DeiT, CLIP vision encoder.</p>
          <div class="tag-list"><span class="tag">ViT</span><span class="tag">DeiT</span><span class="tag">Patches</span><span class="tag">CLIP</span></div>
        </div>
        <div class="card">
          <span class="card-icon">üåä</span>
          <div class="card-title">Diffusion Models</div>
          <p>Forward/reverse diffusion, score matching, DDPM, DDIM. Latent diffusion (Stable Diffusion). Classifier-free guidance. The foundation of modern generative vision.</p>
          <div class="tag-list"><span class="tag">DDPM</span><span class="tag">Latent Diffusion</span><span class="tag">Guidance</span></div>
        </div>
        <div class="card">
          <span class="card-icon">üîó</span>
          <div class="card-title">Multimodal Alignment</div>
          <p>CLIP: contrastive vision-language learning. BLIP, LLaVA: vision-language models. Understand how alignment enables zero-shot transfer and emergent reasoning.</p>
          <div class="tag-list"><span class="tag">CLIP</span><span class="tag">BLIP</span><span class="tag">LLaVA</span><span class="tag">VQA</span></div>
        </div>
        <div class="card">
          <span class="card-icon">üé®</span>
          <div class="card-title">3D &amp; Geometric Deep Learning</div>
          <p>NeRF, 3D Gaussian Splatting. Graph Neural Networks for 3D point clouds. Equivariant networks. The frontier of spatial reasoning.</p>
          <div class="tag-list"><span class="tag">NeRF</span><span class="tag">3DGS</span><span class="tag">GNN</span><span class="tag">Equivariance</span></div>
        </div>
        <div class="card">
          <span class="card-icon">üé¨</span>
          <div class="card-title">Video Understanding</div>
          <p>Temporal convolutions, 3D CNNs, optical flow. Transformer-based video models. Video-language alignment. Sora / world models on the horizon.</p>
          <div class="tag-list"><span class="tag">3D CNN</span><span class="tag">Optical Flow</span><span class="tag">Video-LM</span></div>
        </div>
      </div>

      <div class="table-card">
        <div class="table-card-header">
          <div class="dot-indicator"></div>
          <span>Core Papers ‚Äî Phase 2 Reading List</span>
        </div>
        <table>
          <tr><th>Year</th><th>Paper</th><th>Significance</th></tr>
          <tr><td>2012</td><td>AlexNet (Krizhevsky et al.)</td><td>Deep learning revolution in vision</td></tr>
          <tr><td>2015</td><td>ResNet (He et al.)</td><td>Skip connections; enabled very deep networks</td></tr>
          <tr><td>2020</td><td>Vision Transformer (Dosovitskiy et al.)</td><td>Transformers work in vision without inductive bias</td></tr>
          <tr><td>2020</td><td>CLIP (Radford et al.)</td><td>Vision-language alignment; zero-shot transfer</td></tr>
          <tr><td>2020</td><td>Denoising Diffusion Probabilistic Models (Ho et al.)</td><td>Modern generative model foundation</td></tr>
          <tr><td>2021</td><td>NeRF (Mildenhall et al.)</td><td>Neural radiance fields; 3D from 2D</td></tr>
          <tr><td>2023</td><td>Stable Diffusion (Rombach et al.)</td><td>Latent diffusion; democratized image generation</td></tr>
          <tr><td>2023</td><td>LLaVA (Liu et al.)</td><td>Vision-language instruction tuning</td></tr>
          <tr><td>2024</td><td>3D Gaussian Splatting (Kerbl et al.)</td><td>Real-time 3D rendering alternative to NeRF</td></tr>
          <tr><td>2024</td><td>GPT-4V / Gemini Vision Reports</td><td>State-of-the-art multimodal reasoning</td></tr>
        </table>
      </div>

      <div class="cards-grid">
        <div class="card">
          <div class="card-title">üõ† Practical Projects ‚Äî Phase 2</div>
          <ul class="checklist">
            <li><span class="check-box"></span>Implement ResNet from scratch; train on CIFAR-10</li>
            <li><span class="check-box"></span>Build Vision Transformer (ViT) and fine-tune on ImageNet</li>
            <li><span class="check-box"></span>Implement DDPM diffusion model; generate images</li>
            <li><span class="check-box"></span>Reproduce CLIP training on a small dataset</li>
            <li><span class="check-box"></span>Fine-tune a vision-language model (LLaVA) on custom data</li>
            <li><span class="check-box"></span>Implement NeRF or 3D Gaussian Splatting</li>
          </ul>
        </div>
        <div class="card">
          <div class="card-title">üìö Courses</div>
          <ul class="checklist">
            <li><span class="check-box"></span><strong><a href="https://cs231n.stanford.edu/" target="_blank" rel="noopener noreferrer">Stanford CS231N</a></strong> ‚Äî Deep Learning for Computer Vision (primary)</li>
            <li><span class="check-box"></span><strong>fast.ai Part 2</strong> ‚Äî Advanced Deep Learning (applications)</li>
            <li><span class="check-box"></span><strong>Hugging Face Diffusers course</strong> ‚Äî Hands-on diffusion models</li>
            <li><span class="check-box"></span><strong>CVPR / ICCV tutorials</strong> ‚Äî Latest vision research</li>
          </ul>
        </div>
      </div>

      <div style="margin-top:16px">
        <div class="progress-wrap">
          <div class="progress-label"><span>Phase Progress</span><span>60%</span></div>
          <div class="progress-bar"><div class="progress-fill" style="width:60%"></div></div>
        </div>
      </div>
    </section>

    <!-- ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê -->
    <!-- PHASE 3 ‚Äî SYSTEMS & REASONING -->
    <!-- ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê -->
    <section class="p3">
      <div class="phase-header">
        <div class="phase-dot">3</div>
        <div class="phase-title-wrap">
          <div class="phase-label">// Phase Three ¬∑ Months 13‚Äì22</div>
          <div class="phase-title">Systems, Reasoning &amp; Agents</div>
        </div>
      </div>

      <div class="milestone">
        <div class="milestone-icon">üéØ</div>
        <div>
          <div class="milestone-title">Goal</div>
          <div class="milestone-text">Master GPU programming, distributed training, and advanced reasoning. Build systems that scale. Implement reasoning chains, reinforcement learning for alignment, and agentic systems.</div>
        </div>
      </div>

      <div class="cards-grid">
        <div class="card">
          <span class="card-icon">‚öôÔ∏è</span>
          <div class="card-title">GPU &amp; Systems Programming</div>
          <p>CUDA fundamentals, memory hierarchy, kernel optimization. Triton for custom kernels. FlashAttention, FlashDecoding. Distributed training: DDP, FSDP, tensor parallelism.</p>
          <div class="tag-list"><span class="tag">CUDA</span><span class="tag">Triton</span><span class="tag">FlashAttn</span><span class="tag">FSDP</span></div>
        </div>
        <div class="card">
          <span class="card-icon">üß†</span>
          <div class="card-title">Reasoning &amp; Chain-of-Thought</div>
          <p>CoT prompting, Tree-of-Thought, Self-Consistency. Scaling test-time compute. Process reward models vs outcome reward models. Reasoning as search.</p>
          <div class="tag-list"><span class="tag">CoT</span><span class="tag">ToT</span><span class="tag">Process Rewards</span></div>
        </div>
        <div class="card">
          <span class="card-icon">üéÆ</span>
          <div class="card-title">Reinforcement Learning</div>
          <p>Policy gradients, actor-critic, PPO, A3C. RLHF: reward modeling, SFT + RL pipeline. DPO: direct preference optimization. GRPO: group relative policy optimization.</p>
          <div class="tag-list"><span class="tag">PPO</span><span class="tag">RLHF</span><span class="tag">DPO</span><span class="tag">GRPO</span></div>
        </div>
        <div class="card">
          <span class="card-icon">ü§ñ</span>
          <div class="card-title">Agentic Systems</div>
          <p>ReAct: Reasoning + Acting. Tool use, function calling, code execution. Planning and hierarchical reasoning. Multi-agent systems. Autonomous workflows.</p>
          <div class="tag-list"><span class="tag">ReAct</span><span class="tag">Tool Use</span><span class="tag">Planning</span><span class="tag">Multi-Agent</span></div>
        </div>
        <div class="card">
          <span class="card-icon">üîç</span>
          <div class="card-title">RAG &amp; Knowledge Grounding</div>
          <p>Retrieval-Augmented Generation: dense retrieval, hybrid search, re-ranking. Advanced RAG: HyDE, RAPTOR, agentic RAG. Reducing hallucinations in production.</p>
          <div class="tag-list"><span class="tag">RAG</span><span class="tag">FAISS</span><span class="tag">HyDE</span><span class="tag">RAPTOR</span></div>
        </div>
        <div class="card">
          <span class="card-icon">üé¨</span>
          <div class="card-title">Video & World Models</div>
          <p>Sora 2 / Sora 3 architecture (Diffusion Transformer). VideoThinkBench: video generation models for spatial reasoning. Runway Gen-4.5 physics simulation.</p>
          <div class="tag-list"><span class="tag">DiT</span><span class="tag">Sora</span><span class="tag">VideoThink</span><span class="tag">World Models</span></div>
        </div>
        <div class="card">
          <span class="card-icon">üî¨</span>
          <div class="card-title">Interpretability &amp; Safety</div>
          <p>Mechanistic interpretability: circuits, superposition, feature visualization. Anthropic's sparse autoencoders. Constitutional AI, Debate, scalable oversight methods.</p>
          <div class="tag-list"><span class="tag">Circuits</span><span class="tag">SAE</span><span class="tag">CAI</span><span class="tag">Oversight</span></div>
        </div>
      </div>

      <div class="table-card">
        <div class="table-card-header">
          <div class="dot-indicator"></div>
          <span>Core Papers ‚Äî Frontier Research (2023‚Äì2026)</span>
        </div>
        <table>
          <tr><th>Year</th><th>Paper</th><th>Significance</th></tr>
          <tr><td>2023</td><td><a href="https://arxiv.org/abs/2312.00752" target="_blank" rel="noopener noreferrer">Mamba: Linear-Time Sequence Modeling</a></td><td>State space alternative to attention ‚Äî potential successor</td></tr>
          <tr><td>2023</td><td><a href="https://arxiv.org/abs/2302.04761" target="_blank" rel="noopener noreferrer">Toolformer (Schick et al.)</a></td><td>Models teaching themselves to use APIs</td></tr>
          <tr><td>2024</td><td><a href="https://arxiv.org/abs/2501.12948" target="_blank" rel="noopener noreferrer">DeepSeek-R1 (DeepSeek-AI)</a></td><td>Pure RL-trained reasoner ‚Äî new training paradigm</td></tr>
          <tr><td>2024</td><td><a href="https://openai.com/index/hello-gpt-4o/" target="_blank" rel="noopener noreferrer">GPT-4o Technical Report (OpenAI)</a></td><td>Omni-modal native multimodality at scale</td></tr>
          <tr><td>2024</td><td><a href="https://www.swebench.com/" target="_blank" rel="noopener noreferrer">SWE-Bench Verified</a></td><td>Gold standard for software engineering agents</td></tr>
          <tr><td>2025</td><td><a href="https://arxiv.org/abs/2412.19437" target="_blank" rel="noopener noreferrer">DeepSeek-V3 (MoE, open-source)</a></td><td>Open SOTA ‚Äî study training efficiency innovations</td></tr>
          <tr><td>2025</td><td>SWE-RL (Software Engineering RL)</td><td>RL for coding; open-book vs closed-book insight</td></tr>
          <tr><td>2025</td><td>VideoThinkBench</td><td>Video generation surpassing VLMs in spatial reasoning</td></tr>
          <tr><td>2025</td><td>Lumina-Video (V2A synthesis)</td><td>Synchronized video-to-audio generation</td></tr>
          <tr><td>2026</td><td>VGGT + frontier geometry papers</td><td>Next-gen 3D-aware visual reasoning</td></tr>
        </table>
      </div>

      <div class="cards-grid">
        <div class="card">
          <div class="card-title">üõ† Practical Projects ‚Äî Phase 4</div>
          <ul class="checklist">
            <li><span class="check-box"></span>Implement GRPO from scratch; train a math reasoning model</li>
            <li><span class="check-box"></span>Build a multi-step ReAct agent with tool use (search, code exec)</li>
            <li><span class="check-box"></span>Reproduce DeepSeek-R1 RL training on a small task</li>
            <li><span class="check-box"></span>Build an advanced RAG pipeline; benchmark vs naive RAG</li>
            <li><span class="check-box"></span>Run mechanistic interpretability on a 1B model (TransformerLens)</li>
            <li><span class="check-box"></span>Submit to a SWE-Bench leaderboard</li>
            <li><span class="check-box"></span><strong style="color:var(--accent5)">Write and submit a research paper</strong></li>
          </ul>
        </div>
        <div class="card">
          <div class="card-title">üìö Courses &amp; Seminars</div>
          <ul class="checklist">
            <li><span class="check-box"></span><strong><a href="https://cme295.stanford.edu/" target="_blank" rel="noopener noreferrer">Stanford CME295</a></strong> ‚Äî Transformers &amp; LLMs (advanced)</li>
            <li><span class="check-box"></span><strong><a href="http://rail.eecs.berkeley.edu/deeprlcourse/" target="_blank" rel="noopener noreferrer">Berkeley CS285</a></strong> ‚Äî Deep Reinforcement Learning</li>
            <li><span class="check-box"></span><strong><a href="http://www.cs.cmu.edu/~nasmith/ANLPS/" target="_blank" rel="noopener noreferrer">CMU 11-713</a></strong> ‚Äî Advanced NLP (PhD seminar)</li>
            <li><span class="check-box"></span><strong>BAIR / CSAIL reading groups</strong> (join or follow online)</li>
            <li><span class="check-box"></span><strong>NeurIPS / ICML / CVPR</strong> ‚Äî Attend tutorials, workshops</li>
          </ul>
        </div>
      </div>

      <div style="margin-top:16px">
        <div class="progress-wrap">
          <div class="progress-label"><span>Phase Progress</span><span>40%</span></div>
          <div class="progress-bar"><div class="progress-fill" style="width:40%"></div></div>
        </div>
      </div>
    </section>

  </div><!-- /timeline -->

  <!-- SENIOR ENGINEER SKILLS -->
  <div class="section-divider">
    <div class="section-divider-line"></div>
    <div class="section-divider-text">// Senior AI Engineer Skills</div>
    <div class="section-divider-line"></div>
  </div>

  <div style="margin-bottom:16px" class="skills-section">
    <div class="cards-grid">
      <div class="card">
        <span class="card-icon">üêç</span>
        <div class="card-title">Python &amp; ML Stack</div>
        <p>PyTorch (primary), JAX, Triton. Hugging Face Transformers, Datasets, PEFT. Weights &amp; Biases, MLflow for experiment tracking. High-performance Python profiling.</p>
        <div class="tag-list">
          <span class="tag">PyTorch</span><span class="tag">JAX</span><span class="tag">HuggingFace</span><span class="tag">W&amp;B</span>
        </div>
      </div>
      <div class="card">
        <span class="card-icon">‚òÅÔ∏è</span>
        <div class="card-title">Cloud &amp; Infra</div>
        <p>AWS / GCP / Azure GPU instances. Kubernetes for ML workloads. Docker containerization. Terraform infrastructure-as-code. CI/CD for model pipelines.</p>
        <div class="tag-list">
          <span class="tag">AWS</span><span class="tag">Docker</span><span class="tag">K8s</span><span class="tag">CI/CD</span>
        </div>
      </div>
      <div class="card">
        <span class="card-icon">üöÄ</span>
        <div class="card-title">Model Serving &amp; Deployment</div>
        <p>vLLM, TGI (Text Generation Inference), Ollama. ONNX export, TensorRT optimization. Batching strategies, KV cache management, SLA-aware inference.</p>
        <div class="tag-list">
          <span class="tag">vLLM</span><span class="tag">TGI</span><span class="tag">TensorRT</span><span class="tag">KV Cache</span>
        </div>
      </div>
      <div class="card">
        <span class="card-icon">üìè</span>
        <div class="card-title">Evaluation &amp; Benchmarking</div>
        <p>MMLU, HumanEval, MATH, MT-Bench. LLM-as-a-judge frameworks. Red-teaming and safety evaluation. Building custom evals for production use cases.</p>
        <div class="tag-list">
          <span class="tag">MMLU</span><span class="tag">LM-Eval</span><span class="tag">LLM-Judge</span><span class="tag">Red-Teaming</span>
        </div>
      </div>
    </div>
  </div>

  <!-- MASTER TIMELINE -->
  <div class="section-divider">
    <div class="section-divider-line"></div>
    <div class="section-divider-text">// Master Timeline</div>
    <div class="section-divider-line"></div>
  </div>

  <div class="table-card" style="margin-bottom:24px">
    <div class="table-card-header">
      <div class="dot-indicator" style="background:var(--accent1);box-shadow:0 0 8px var(--accent1)"></div>
      <span>36-Month Fast-Track Plan</span>
    </div>
    <table>
      <tr><th>Period</th><th>Phase</th><th>Primary Focus</th><th>Milestone</th></tr>
      <tr><td>M0‚Äì3</td><td>Phase 0</td><td>Math: LinAlg, Calc, Prob, InfoTheory</td><td>Pass ML foundations exam; derive backprop from scratch</td></tr>
      <tr><td>M3‚Äì9</td><td>Phase 1a</td><td>NLP: Transformers, LLMs, Alignment (CS224N)</td><td>Implement Transformer from scratch; fine-tune LLaMA</td></tr>
      <tr><td>M6‚Äì12</td><td>Phase 1b</td><td>Vision: CNNs, ViT, Diffusion, NeRF (CS231N)</td><td>Train diffusion model; implement ViT; reproduce CLIP</td></tr>
      <tr><td>M12‚Äì18</td><td>Phase 2</td><td>Systems: GPU, FlashAttn, Distributed (CS336)</td><td>Train 1B model from scratch; write Triton kernel</td></tr>
      <tr><td>M16‚Äì22</td><td>Phase 3a</td><td>Reasoning: CoT, RLHF, DPO, GRPO</td><td>Implement DPO pipeline; reproduce DeepSeek-R1 on small task</td></tr>
      <tr><td>M20‚Äì28</td><td>Phase 3b</td><td>Agents: ReAct, RAG, Tool Use, Safety</td><td>SWE-Bench submission; build production RAG system</td></tr>
      <tr><td>M24‚Äì36</td><td>Phase 4</td><td>Research Frontier + Original Contribution</td><td>First paper accepted at top-tier conference (NeurIPS/CVPR/ICLR)</td></tr>
    </table>
  </div>

  <!-- OUTCOMES -->
  <div class="section-divider">
    <div class="section-divider-line"></div>
    <div class="section-divider-text">// End State</div>
    <div class="section-divider-line"></div>
  </div>

  <div class="outcome-grid">
    <div class="outcome-card oc1">
      <div class="outcome-icon">üéì</div>
      <div class="outcome-title">PhD-Level Theory</div>
      <div class="outcome-text">Can pass qualifying exams at CMU, MIT, Stanford. Fluent in the full literature from HMMs to Mamba. Capable of reading and critiquing any new paper at submission.</div>
    </div>
    <div class="outcome-card oc2">
      <div class="outcome-icon">‚öôÔ∏è</div>
      <div class="outcome-title">Senior Engineer</div>
      <div class="outcome-text">Trains and deploys production models. Writes custom GPU kernels. Builds agentic pipelines, RAG systems, and evaluation frameworks. Proficient in distributed training at scale.</div>
    </div>
    <div class="outcome-card oc3">
      <div class="outcome-icon">üî¨</div>
      <div class="outcome-title">Researcher</div>
      <div class="outcome-text">Publishes at CVPR, NeurIPS, ICLR, or ACL. Contributes to open-source frontier models. Identifies and solves novel problems at the intersection of NLP, vision, and reasoning.</div>
    </div>
  </div>

  <!-- KEY INSIGHT BOX -->
  <div class="insight-box">
    <div class="insight-label">// Key Insight: The Fast-Track Strategy</div>
    <div class="insight-text">
      Phases 1 and 2 (NLP + Vision) overlap intentionally ‚Äî both rely on the same Transformer backbone. Once you deeply understand one, the other accelerates. The most common trap is spending too long on theory before shipping projects. <span class="highlight">Ship something every two weeks.</span> The research frontier is won by those who can both read the paper and run the experiment.
    </div>
  </div>

  <footer>
    <p>AI/ML RESEARCHER ROADMAP // PHASED EXECUTION PLAN // 2025‚Äì2028</p>
    <p style="margin-top:8px;opacity:0.5">Based on Stanford CS224N ¬∑ CS231N ¬∑ CS336 ¬∑ CME295 ¬∑ MIT 6.869 ¬∑ CMU 11-713 ¬∑ Berkeley CS285</p>
  </footer>

</div>

<script>
  const html = document.documentElement;
  const btn = document.getElementById('themeToggle');
  const icon = document.getElementById('toggleIcon');
  const label = document.getElementById('toggleLabel');

  // Restore saved preference
  const saved = localStorage.getItem('theme') || 'dark';
  setTheme(saved);

  btn.addEventListener('click', () => {
    const current = html.getAttribute('data-theme');
    setTheme(current === 'dark' ? 'light' : 'dark');
  });

  function setTheme(t) {
    html.setAttribute('data-theme', t);
    localStorage.setItem('theme', t);
    if (t === 'light') {
      icon.textContent = 'üåô';
      label.textContent = 'Dark';
    } else {
      icon.textContent = '‚òÄÔ∏è';
      label.textContent = 'Light';
    }
  }
</script>

</body>
</html>
